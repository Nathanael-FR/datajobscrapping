---
- name: Pull images and deploy containers on VPS
  hosts: vps
  gather_facts: false
  become: true

  tasks:

    ###### Clean up unused images ######
    - name: Remove ununsed images (prune -a)
      shell : docker image prune -a -f

    ###### Pull images from Dockerhub ######
    - name: Pull Postgres image 
      community.docker.docker_image:
        name: "{{ dockerhub_username }}/mypostgres:{{ push_id }}"
        source: pull
    
    - name: Pull Webscraper image
      community.docker.docker_image:
        name: "{{ dockerhub_username }}/webscraper:{{ push_id }}"
        source: pull

    - name: Pull Airflow image
      community.docker.docker_image:
        name: "{{ dockerhub_username }}/airflow:{{ push_id }}"
        source: pull

    ###### Create volumes and networks ######
    - name: Create a volume for Postgres data if not exists
      community.docker.docker_volume:
        name: postgres_data
        state: present

    - name: Create a volume for Airflow data if not exists
      community.docker.docker_volume:
        name: airflow_data
        state: present

    - name: Create webscraper network for the containers if not exists
      community.docker.docker_network:
        name: webscraper_network
        state: present

    - name: Create airflow network for the containers if not exists
      community.docker.docker_network:
        name: airflow_network
        state: present

    ###### Stop and remove existing containers ######
    - name: Stop and remove existing Postgres container
      community.docker.docker_container:
        name: mypostgres
        state: absent

    - name: Stop and remove existing Webscraper container
      community.docker.docker_container:
        name: webscraper
        state: absent

    - name: Stop and remove existing Airflow containers
      community.docker.docker_container:
        name: airflow_webserver
        state: absent


    ###### Run Postgres, Webscraper and Airflow containers ######
    - name: Run Postgres container on VPS
      community.docker.docker_container:
        name: mypostgres
        image: "{{ dockerhub_username }}/mypostgres:{{ push_id }}"
        state: started
        restart_policy: always
        volumes:
          - postgres_data:/var/lib/postgresql/data
        env:
          POSTGRES_USER: "{{ postgres_user }}"
          POSTGRES_PASSWORD: "{{ postgres_password }}"
          POSTGRES_DB: "{{ postgres_db }}"
        networks:
          - name: webscraper_network
        healthcheck:
          test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER}"]
          interval: 5s
          timeout: 5s
          retries: 5
    
    - name: Wait for Postgres container to be healthy
      community.docker.docker_container_info:
        name: mypostgres
      register: postgres_container_info
      until: postgres_container_info.container.State.Health.Status == "healthy"
      retries: 10
      delay: 5

    - name: Run Webscraper containers on VPS
      community.docker.docker_container:
        name: webscraper
        image: "{{ dockerhub_username }}/webscraper:{{ push_id }}"
        state: started
        restart_policy: always
        image_name_mismatch: recreate
        env:
          AWS_ACCESS_KEY_ID: "{{ aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ aws_secret_access_key }}"
          S3_BUCKET_NAME: "{{ s3_bucket_name }}"
          DB_HOST: "mypostgres"
          DB_PORT: "5432"
          DB_NAME: "{{ postgres_db }}"
        networks:
          - name: webscraper_network

    - name: Run Airflow postgres container on VPS
      community.docker.docker_container:
        name: airflow_postgres
        image: postgres:14.1-alpine
        state: started
        restart_policy: always
        volumes:
          - airflow_data:/var/lib/postgresql/data
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        networks:
          - name: airflow_network
        healthcheck:
          test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER}"]
          interval: 5s
          timeout: 5s
          retries: 5

    - name: Wait for Airflow's Postgres container to be healthy
      community.docker.docker_container_info:
        name: airflow_postgres
      register: airflow_postgres_container_info
      until: airflow_postgres_container_info.container.State.Health.Status == "healthy"
      retries: 10
      delay: 5


    - name: Run Airflow webserver container on VPS
      community.docker.docker_container:
        name: airflow_webserver
        image: "{{ dockerhub_username }}/airflow:{{ push_id }}"
        state: started
        restart_policy: always
        env:
          AIRFLOW__CORE__EXECUTOR: LocalExecutor
          AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres/airflow
          AIRFLOW__CORE__FERNET_KEY: 'L_x43MRdzsoGOzCGwqcLhcpOZ6fQMrvNw_AT89oKdl4='
          AIRFLOW__WEBSERVER__SECRET_KEY: 'YOUR_SECRET_KEY_HERE'
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          AWS_ACCESS_KEY_ID: "{{ aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ aws_secret_access_key }}"
          S3_BUCKET_NAME: "{{ s3_bucket_name }}"
        networks:
          - name: airflow_network
        ports:
          - 8082:8080
        healthcheck:
          test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
          interval: 30s
          timeout: 30s
          retries: 3
        command: ["airflow", "webserver"]

    - name: Run Airflow scheduler container on VPS
      community.docker.docker_container:
        name: airflow_scheduler
        image: "{{ dockerhub_username }}/airflow:{{ push_id }}"
        state: started
        restart_policy: always
        env:
          AIRFLOW__CORE__EXECUTOR: LocalExecutor
          AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres/airflow
          AIRFLOW__CORE__FERNET_KEY: 'L_x43MRdzsoGOzCGwqcLhcpOZ6fQMrvNw_AT89oKdl4='
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
        networks:
          - name: airflow_network
        volumes:
          - ./airflow/dags:/opt/airflow/dags
          - ./airflow/utils:/opt/airflow/utils
        command: ["airflow", "scheduler"]

    ###### Check if the containers are running ######
    - name: Check if Webscraper container is running
      community.docker.docker_container_info:
        name: webscraper
      register: webscraper_container

    - name: Check if Airflow webserver container is running
      community.docker.docker_container_info:
        name: airflow_webserver
      register: airflow_webserver_container

    - name: Check if Airflow scheduler container is running
      community.docker.docker_container_info:
        name: airflow_scheduler
      register: airflow_scheduler_container

    ###### Assert that the containers are running ######

    - name: Ensure Webscraper container is running
      assert:
        that:
          - webscraper_container.containers is not none
          - webscraper_container.containers[0].State.Running
        fail_msg: "Webscraper container is not running!"
      ignore_errors: yes

    - name: Ensure Airflow webserver container is running
      assert:
        that:
          - airflow_webserver_container.containers is not none
          - airflow_webserver_container.containers[0].State.Running
        fail_msg: "Airflow webserver container is not running!"
      ignore_errors: yes

    - name: Ensure Airflow scheduler container is running
      assert:
        that:
          - airflow_scheduler_container.containers is not none
          - airflow_scheduler_container.containers[0].State.Running
        fail_msg: "Airflow scheduler container is not running!"
      ignore_errors: yes

    ###### Check environment variables inside the containers ######
    - name: Check environment variables inside the webscraper container
      shell: |
        docker exec webscraper python3 -c "import os; assert os.environ['AWS_ACCESS_KEY_ID'] == '{{ aws_access_key_id }}'"
        docker exec webscraper python3 -c "import os; assert os.environ['AWS_SECRET_ACCESS_KEY'] == '{{ aws_secret_access_key }}'"
        docker exec webscraper python3 -c "import os; assert os.environ['S3_BUCKET_NAME'] == '{{ s3_bucket_name }}'"
      ignore_errors: yes

    - name: Check environment variables inside the airflow webserver container
      shell: |
        docker exec airflow_webserver python3 -c "import os; assert os.environ['AWS_ACCESS_KEY_ID'] == '{{ aws_access_key_id }}'"
        docker exec airflow_webserver python3 -c "import os; assert os.environ['AWS_SECRET_ACCESS_KEY'] == '{{ aws_secret_access_key }}'"
        docker exec airflow_webserver python3 -c "import os; assert os.environ['S3_BUCKET_NAME'] == '{{ s3_bucket_name }}'"
      ignore_errors: yes

    ###### Clean up unused images ######

    - name: Remove ununsed images (prune -a)
      shell : docker image prune -a -f
      